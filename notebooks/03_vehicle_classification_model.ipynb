{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baeebee",
   "metadata": {},
   "source": [
    "# AAI-521 Final Project – Group 3  \n",
    "## 03 – Vehicle Classification Model (LMV vs HMV)\n",
    "\n",
    "**Goal:**  \n",
    "Train and evaluate a convolutional neural network (CNN) using the cropped\n",
    "vehicle dataset created in Notebook 02.\n",
    "\n",
    "This notebook includes:\n",
    "- Loading preprocessed crops and labels\n",
    "- Train/validation split\n",
    "- CNN architecture\n",
    "- Training loop and learning curves\n",
    "- Evaluation (accuracy, classification report, confusion matrix)\n",
    "- Qualitative results (annotated frames and video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc18ab3",
   "metadata": {},
   "source": [
    "#### Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9851c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "IMAGES_ROOT = DATA_ROOT / \"DETRAC-Images\"\n",
    "TRAIN_ANN_ROOT = DATA_ROOT / \"DETRAC-Train-Annotations\"\n",
    "CROPPED_DATA_PATH = PROJECT_ROOT / \"outputs\" / \"cropped_vehicle_dataset.npz\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Images root:\", IMAGES_ROOT)\n",
    "print(\"Train annotations root:\", TRAIN_ANN_ROOT)\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Loading dataset from:\", CROPPED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fc207",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(CROPPED_DATA_PATH, allow_pickle=True)\n",
    "images = data[\"images\"]  # (N, H, W, 3), float32 in [0,1]\n",
    "labels = data[\"labels\"]  # (N,)\n",
    "class_to_idx = data[\"class_to_idx\"].item()\n",
    "metadata = data[\"metadata\"]\n",
    "\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Classes:\", idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574359b5",
   "metadata": {},
   "source": [
    "## 1. Train / Validation Split\n",
    "\n",
    "We split the cropped dataset into training and validation sets\n",
    "using an 80/20 random split with a fixed random seed for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689ec86",
   "metadata": {},
   "source": [
    "#### Prepare Tensors & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(images).permute(0, 3, 1, 2)  # (N,3,H,W)\n",
    "y = torch.from_numpy(labels)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_ratio = 0.8\n",
    "n_total = len(dataset)\n",
    "n_train = int(train_ratio * n_total)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b25f6a",
   "metadata": {},
   "source": [
    "## 2. CNN Architecture\n",
    "\n",
    "We use a simple convolutional neural network with four convolutional\n",
    "blocks followed by two fully connected layers.\n",
    "\n",
    "This is our **baseline** model for vehicle classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2e3d8",
   "metadata": {},
   "source": [
    "#### CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661673c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 256),  # adjust if our feature map size differs\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(class_to_idx)\n",
    "model = VehicleClassifier(num_classes=num_classes).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c5630",
   "metadata": {},
   "source": [
    "#### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698a09d",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "\n",
    "We train for a fixed number of epochs and track:\n",
    "\n",
    "- Training and validation loss\n",
    "- Training and validation accuracy\n",
    "\n",
    "We keep the model weights that achieved the best validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93794001",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf17f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    n_train = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_correct += (preds == yb).sum().item()\n",
    "        n_train += xb.size(0)\n",
    "\n",
    "    train_loss /= n_train\n",
    "    train_acc = train_correct / n_train\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    n_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            n_val += xb.size(0)\n",
    "\n",
    "    val_loss /= n_val\n",
    "    val_acc = val_correct / n_val\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{NUM_EPOCHS} \"\n",
    "        f\"- train_loss: {train_loss:.4f}, train_acc: {train_acc:.3f} \"\n",
    "        f\"- val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.3f}\")\n",
    "model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fa4a5",
   "metadata": {},
   "source": [
    "## 4. Learning Curves\n",
    "\n",
    "We plot training and validation loss and accuracy across epochs.\n",
    "These figures will be included in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d963a",
   "metadata": {},
   "source": [
    "#### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bec4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ----- LOSS PLOT -----\n",
    "axes[0].plot(epochs, history[\"train_loss\"], label=\"Train\", marker=\"o\")\n",
    "axes[0].plot(epochs, history[\"val_loss\"], label=\"Validation\", marker=\"o\")\n",
    "axes[0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Cross-Entropy Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "# ----- ACCURACY PLOT -----\n",
    "axes[1].plot(epochs, history[\"train_acc\"], label=\"Train\", marker=\"o\")\n",
    "axes[1].plot(epochs, history[\"val_acc\"], label=\"Validation\", marker=\"o\")\n",
    "axes[1].set_title(\"Training vs Validation Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0.8, 1.01)  # adjust if needed\n",
    "axes[1].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b454733",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"vehicle_classifier.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(\"Saved best model to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a08404",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Validation Set\n",
    "\n",
    "We compute:\n",
    "- Overall accuracy\n",
    "- Classification report (precision, recall, F1)\n",
    "- Normalized confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ab30b",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06840",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(yb.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "print(classification_report(\n",
    "    all_targets, all_preds,\n",
    "    target_names=[idx_to_class[i] for i in range(num_classes)]\n",
    "))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    all_targets, all_preds,\n",
    "    display_labels=[idx_to_class[i] for i in range(num_classes)],\n",
    "    normalize=\"true\",\n",
    "    ax=ax,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "ax.set_title(\"Normalized Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf42e6",
   "metadata": {},
   "source": [
    "## 6. Qualitative Results – Annotated Frames\n",
    "\n",
    "We apply the trained CNN on original DETRAC frames to visualize its\n",
    "predictions per vehicle bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac679f8",
   "metadata": {},
   "source": [
    "#### Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# from src.utils_detrac import load_detrac_annotations\n",
    "def load_detrac_annotations(xml_path: Path):\n",
    "    \"\"\"\n",
    "    Parse a UA-DETRAC XML file into a dict:\n",
    "    { frame_num (int): [ { 'id': str, 'bbox': [x, y, w, h], 'class': str }, ... ] }\n",
    "    \"\"\"\n",
    "    tree = ET.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    annotations = {}\n",
    "\n",
    "    for frame in root.findall(\"frame\"):\n",
    "        frame_num = int(frame.get(\"num\"))\n",
    "\n",
    "        target_list = frame.find(\"target_list\")\n",
    "        if target_list is None:\n",
    "            annotations[frame_num] = []\n",
    "            continue\n",
    "\n",
    "        targets = []\n",
    "        for target in target_list.findall(\"target\"):\n",
    "            tid = target.get(\"id\")\n",
    "\n",
    "            box = target.find(\"box\")\n",
    "            attr = target.find(\"attribute\")\n",
    "            if box is None or attr is None:\n",
    "                continue\n",
    "\n",
    "            left = float(box.get(\"left\"))\n",
    "            top = float(box.get(\"top\"))\n",
    "            width = float(box.get(\"width\"))\n",
    "            height = float(box.get(\"height\"))\n",
    "            vehicle_class = attr.get(\"vehicle_type\")\n",
    "\n",
    "            targets.append({\n",
    "                \"id\": tid,\n",
    "                \"bbox\": [left, top, width, height],\n",
    "                \"class\": vehicle_class,\n",
    "            })\n",
    "\n",
    "        annotations[frame_num] = targets\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# define TARGET_SIZE using the cropped dataset shape (to match training)\n",
    "# images: (N, H, W, 3)\n",
    "crop_h, crop_w = images.shape[1], images.shape[2]\n",
    "TARGET_SIZE = (crop_w, crop_h)  # (width, height) for cv2.resize\n",
    "print(\"CNN input size (W,H):\", TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacdeff",
   "metadata": {},
   "source": [
    "#### predict class for a single crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94aa367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_vehicle_class(crop_rgb_np, model, device=DEVICE):\n",
    "    \"\"\"\n",
    "    crop_rgb_np: numpy array (H, W, 3) in [0, 1] or [0, 255]\n",
    "    Returns: (pred_label_str, confidence_float)\n",
    "    \"\"\"\n",
    "    # Ensure float32 [0,1]\n",
    "    if crop_rgb_np.dtype != \"float32\":\n",
    "        crop = crop_rgb_np.astype(\"float32\")\n",
    "    else:\n",
    "        crop = crop_rgb_np.copy()\n",
    "\n",
    "    if crop.max() > 1.0:\n",
    "        crop /= 255.0\n",
    "\n",
    "    # (H,W,3) -> (1,3,H,W) tensor\n",
    "    tensor = torch.from_numpy(crop).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred_idx = probs.max(dim=1)\n",
    "\n",
    "    pred_idx = int(pred_idx.item())\n",
    "    conf = float(conf.item())\n",
    "    pred_label = idx_to_class[pred_idx]\n",
    "\n",
    "    return pred_label, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a8ca7",
   "metadata": {},
   "source": [
    "#### annotate a single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame(\n",
    "    seq_id: str,\n",
    "    frame_num: int,\n",
    "    model,\n",
    "    images_root=IMAGES_ROOT,\n",
    "    ann_root=TRAIN_ANN_ROOT,\n",
    "    target_size=TARGET_SIZE,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a DETRAC frame + its annotations, run CNN on each bbox crop,\n",
    "    and overlay predicted labels on the original frame.\n",
    "    Returns: annotated RGB image as numpy array.\n",
    "    \"\"\"\n",
    "    seq_images_dir = images_root / seq_id\n",
    "    xml_path = ann_root / f\"{seq_id}.xml\"\n",
    "\n",
    "    assert seq_images_dir.exists(), f\"Image folder not found: {seq_images_dir}\"\n",
    "    assert xml_path.exists(), f\"XML file not found: {xml_path}\"\n",
    "\n",
    "    annotations = load_detrac_annotations(xml_path)\n",
    "\n",
    "    img_file = seq_images_dir / f\"img{frame_num:05d}.jpg\"\n",
    "    assert img_file.exists(), f\"Image not found: {img_file}\"\n",
    "\n",
    "    # Load original frame (BGR -> RGB)\n",
    "    frame_bgr = cv2.imread(str(img_file))\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h_img, w_img = frame_rgb.shape[:2]\n",
    "\n",
    "    # Copy for drawing\n",
    "    vis = frame_rgb.copy()\n",
    "\n",
    "    # Iterate over targets in this frame\n",
    "    targets = annotations.get(frame_num, [])\n",
    "    if not targets:\n",
    "        print(f\"No vehicles found in frame {frame_num}\")\n",
    "        return vis\n",
    "\n",
    "    for t in targets:\n",
    "        x, y, w, h = t[\"bbox\"]\n",
    "\n",
    "        # Clamp bbox to image bounds\n",
    "        x1 = max(int(x), 0)\n",
    "        y1 = max(int(y), 0)\n",
    "        x2 = min(int(x + w), w_img)\n",
    "        y2 = min(int(y + h), h_img)\n",
    "\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        # Crop and resize to CNN input size\n",
    "        crop = frame_rgb[y1:y2, x1:x2]\n",
    "        crop_resized = cv2.resize(crop, target_size)\n",
    "\n",
    "        # Predict class with the CNN\n",
    "        pred_label, conf = predict_vehicle_class(crop_resized, model, device=device)\n",
    "\n",
    "        # Draw bbox\n",
    "        rect = plt.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            fill=False,\n",
    "            edgecolor=\"lime\",\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "\n",
    "        # We draw using matplotlib later, so just store rect and text info\n",
    "        # But here for simplicity, we will immediately draw via OpenCV on vis.\n",
    "        # Use OpenCV drawing for consistency:\n",
    "        vis_bgr = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)\n",
    "        cv2.rectangle(vis_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label_text = f\"{pred_label} ({conf:.2f})\"\n",
    "        cv2.putText(\n",
    "            vis_bgr,\n",
    "            label_text,\n",
    "            (x1, max(y1 - 5, 0)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 0),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        vis = cv2.cvtColor(vis_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de37edc",
   "metadata": {},
   "source": [
    "#### Show one annotated frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sequence and frame to visualize\n",
    "SEQ_ID = \"MVI_20011\"   # adjust to any available sequence\n",
    "FRAME_NUM = 1          # any frame number that exists\n",
    "\n",
    "annotated = annotate_frame(SEQ_ID, FRAME_NUM, model)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(annotated)\n",
    "plt.title(f\"{SEQ_ID} – Frame {FRAME_NUM} (Predicted Labels)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ba12b",
   "metadata": {},
   "source": [
    "## 7. (Optional) Qualitative Results – Annotated Video\n",
    "\n",
    "We generate a short video clip with predicted labels overlaid on each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c079090",
   "metadata": {},
   "source": [
    "#### annotate a frame in BGR for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c197a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame_bgr_for_video(\n",
    "    seq_id: str,\n",
    "    frame_num: int,\n",
    "    model,\n",
    "    images_root=IMAGES_ROOT,\n",
    "    ann_root=TRAIN_ANN_ROOT,\n",
    "    target_size=TARGET_SIZE,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Similar to annotate_frame, but returns annotated frame in BGR format,\n",
    "    suitable for cv2.VideoWriter.\n",
    "    \"\"\"\n",
    "    seq_images_dir = images_root / seq_id\n",
    "    xml_path = ann_root / f\"{seq_id}.xml\"\n",
    "\n",
    "    annotations = load_detrac_annotations(xml_path)\n",
    "\n",
    "    img_file = seq_images_dir / f\"img{frame_num:05d}.jpg\"\n",
    "    if not img_file.exists():\n",
    "        return None\n",
    "\n",
    "    frame_bgr = cv2.imread(str(img_file))\n",
    "    if frame_bgr is None:\n",
    "        return None\n",
    "\n",
    "    h_img, w_img = frame_bgr.shape[:2]\n",
    "    vis = frame_bgr.copy()\n",
    "\n",
    "    targets = annotations.get(frame_num, [])\n",
    "    for t in targets:\n",
    "        x, y, w, h = t[\"bbox\"]\n",
    "\n",
    "        x1 = max(int(x), 0)\n",
    "        y1 = max(int(y), 0)\n",
    "        x2 = min(int(x + w), w_img)\n",
    "        y2 = min(int(y + h), h_img)\n",
    "\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        crop = vis[y1:y2, x1:x2]\n",
    "        crop_resized = cv2.resize(crop, target_size)\n",
    "        crop_rgb = cv2.cvtColor(crop_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        pred_label, conf = predict_vehicle_class(crop_rgb, model, device=device)\n",
    "\n",
    "        # Draw bbox + label on vis (BGR)\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label_text = f\"{pred_label} ({conf:.2f})\"\n",
    "        cv2.putText(\n",
    "            vis,\n",
    "            label_text,\n",
    "            (x1, max(y1 - 5, 0)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57551d",
   "metadata": {},
   "source": [
    "#### Generate the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"videos\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEQ_ID = \"MVI_20011\"     # choose a sequence that exists in your data\n",
    "START_FRAME = 1\n",
    "END_FRAME = 150          # e.g., first 150 frames\n",
    "\n",
    "# Probe first frame to get size\n",
    "first_frame_bgr = annotate_frame_bgr_for_video(SEQ_ID, START_FRAME, model)\n",
    "if first_frame_bgr is None:\n",
    "    raise RuntimeError(\"Could not load the first frame to determine video size.\")\n",
    "\n",
    "height, width = first_frame_bgr.shape[:2]\n",
    "fps = 10  # choose any reasonable FPS\n",
    "\n",
    "output_path = OUTPUT_DIR / f\"{SEQ_ID}_predictions.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "for frame_num in range(START_FRAME, END_FRAME + 1):\n",
    "    frame_bgr = annotate_frame_bgr_for_video(SEQ_ID, frame_num, model)\n",
    "    if frame_bgr is None:\n",
    "        print(f\"Skipping frame {frame_num} (not found or unreadable).\")\n",
    "        continue\n",
    "    writer.write(frame_bgr)\n",
    "\n",
    "writer.release()\n",
    "print(\"Wrote annotated video to:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68653334",
   "metadata": {},
   "source": [
    "#### Display the video inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb793a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(str(output_path), embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f90eb9",
   "metadata": {},
   "source": [
    "#### Convert MP4 → GIF and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659688ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "\n",
    "gif_path = output_path.with_suffix(\".gif\")\n",
    "print(\"Source MP4:\", output_path)\n",
    "print(\"GIF will be saved as:\", gif_path)\n",
    "\n",
    "# Read frames from MP4 and collect for GIF\n",
    "cap = cv2.VideoCapture(str(output_path))\n",
    "frames = []\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ok, frame_bgr = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Optional: take every other frame (reduces GIF size)\n",
    "    if frame_count % 2 != 0:\n",
    "        continue\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(frame_rgb)\n",
    "\n",
    "cap.release()\n",
    "print(f\"Collected {len(frames)} frames for GIF.\")\n",
    "\n",
    "# Save GIF\n",
    "if frames:\n",
    "    imageio.mimsave(gif_path, frames, fps=10)\n",
    "    print(\"GIF saved to:\", gif_path)\n",
    "else:\n",
    "    print(\"No frames collected — check video file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903bcec",
   "metadata": {},
   "source": [
    "#### Display the GIF Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Displaying GIF:\", gif_path)\n",
    "display(Image(filename=str(gif_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc40a2",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "- Trained a CNN for vehicle type classification on cropped DETRAC images.\n",
    "- Achieved validation accuracy of **X%** (see above).\n",
    "- Class-wise performance is summarized in the classification report and\n",
    "  confusion matrix.\n",
    "- Qualitative visualizations show that the model generally predicts\n",
    "  reasonable labels on unseen frames.\n",
    "\n",
    "In future work, we could:\n",
    "- Use a pretrained backbone (ResNet, MobileNet) for better accuracy.\n",
    "- Perform LMV vs HMV grouping and analyze traffic counts over time.\n",
    "- Integrate detection and tracking for full vehicle-counting analytics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
